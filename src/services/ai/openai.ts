/**
 * OpenAI APIã¨ã®çµ±åˆã‚µãƒ¼ãƒ“ã‚¹
 * LangGraphã‚’ä½¿ç”¨ã—ã¦AIã¨ã®å¯¾è©±ã‚’å‡¦ç†
 */

import type { StructuredToolInterface } from "@langchain/core/tools";
import { Message } from "../../types/chat";
import { mcpClient } from "../mcp/client";
import { logDebug, logError } from "../../utils/errorHandler";
import { createChatWorkflow, LangGraphChatWorkflow, ProgressCallback } from "../langgraph/workflow";
import { createKuzuMemoryTools } from "../langgraph/kuzuTools";

const DEFAULT_MODEL = "gpt-4o";

export interface OpenAIMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

export interface OpenAIResponse {
  id: string;
  choices: Array<{
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
}

export class OpenAIService {
  private apiKey: string;
  private model: string;
  private temperature: number = 0.7;
  private maxTokens: number = 1000;
  private customEndpoint?: string;
  private maxToolIterations?: number;
  private progressCallback?: ProgressCallback;
  private workflow: LangGraphChatWorkflow | null = null;
  private memoryTools: StructuredToolInterface[] | null = null;

  constructor(apiKey?: string, model: string = DEFAULT_MODEL) {
    // å¼•æ•° > localStorage > ç’°å¢ƒå¤‰æ•°ã®é †ã§å„ªå…ˆ
    this.apiKey = apiKey || import.meta.env.VITE_OPENAI_API_KEY || "";
    this.model = model;
  }

  /**
   * APIã‚­ãƒ¼ã‚’è¨­å®š
   */
  setApiKey(apiKey: string): void {
    this.apiKey = apiKey;
    this.workflow = null; // APIã‚­ãƒ¼å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
   */
  setModel(model: string): void {
    this.model = model;
    this.workflow = null; // ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * Temperatureã‚’è¨­å®š
   */
  setTemperature(temperature: number): void {
    this.temperature = temperature;
    this.workflow = null; // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨­å®š
   */
  setMaxTokens(maxTokens: number): void {
    this.maxTokens = maxTokens;
    this.workflow = null; // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®šï¼ˆLM Studioç­‰ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ç”¨ï¼‰
   */
  setCustomEndpoint(customEndpoint?: string): void {
    this.customEndpoint = customEndpoint;
    this.workflow = null; // ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * æœ€å¤§ãƒ„ãƒ¼ãƒ«ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°ã‚’è¨­å®š
   */
  setMaxToolIterations(maxToolIterations?: number): void {
    this.maxToolIterations = maxToolIterations;
    this.workflow = null; // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  /**
   * ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š
   */
  setProgressCallback(callback?: ProgressCallback): void {
    this.progressCallback = callback;
    this.workflow = null; // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  }

  private resolveTools(provider: "openai" | "gemini"): StructuredToolInterface[] | undefined {
    if (provider !== "openai") {
      return undefined;
    }

    if (!this.memoryTools) {
      this.memoryTools = createKuzuMemoryTools();
    }

    return this.memoryTools;
  }

  /**
   * LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’åˆæœŸåŒ–
   */
  private initializeWorkflow(provider: "openai" | "gemini", apiKey: string, model: string): void {
    if (!this.workflow) {
      this.workflow = createChatWorkflow(
        provider,
        apiKey,
        model,
        this.temperature,
        this.maxTokens,
        this.customEndpoint,
        this.resolveTools(provider),
        this.maxToolIterations,
        this.progressCallback
      );
    }
  }

  /**
   * LangGraphã‚’ä½¿ç”¨ã—ã¦AI APIã‚’å‘¼ã³å‡ºã™ï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰
   */
  async chat(messages: Message[], provider: "openai" | "gemini" = "openai"): Promise<string> {
    const providerName = provider === "openai" ? "OpenAI" : "Gemini";
    
    // APIã‚­ãƒ¼ã®æ¤œè¨¼
    if (!this.apiKey) {
      throw new Error(
        `${providerName} APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n` +
        `ğŸ’¡ è¨­å®šç”»é¢ï¼ˆæ­¯è»Šã‚¢ã‚¤ã‚³ãƒ³ï¼‰ã‹ã‚‰APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚`
      );
    }

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
    if (!messages || messages.length === 0) {
      throw new Error("é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚");
    }

    try {
      logDebug("AI Service", `LangGraphçµŒç”±ã§${providerName}ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡`, {
        provider: provider,
        messageCount: messages.length,
        model: this.model,
      });

      // MCPçµŒç”±ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ï¼‰
      try {
        await mcpClient.connect();
        await mcpClient.request("context_update", {
          messages: messages,
          timestamp: Date.now(),
        });
      } catch (mcpError) {
        // MCPã‚¨ãƒ©ãƒ¼ã¯è­¦å‘Šã¨ã—ã¦è¨˜éŒ²ã—ã€å‡¦ç†ã¯ç¶šè¡Œ
        const mcpErrorMessage = mcpError instanceof Error ? mcpError.message : String(mcpError);
        logDebug("AI Service", "MCPæ¥ç¶šã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ï¼‰", { 
          error: mcpErrorMessage 
        });
      }

      // LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’åˆæœŸåŒ–
      this.initializeWorkflow(provider, this.apiKey, this.model);

      if (!this.workflow) {
        throw new Error("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
      }

      // LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
      const response = await this.workflow.execute(messages);

      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
      if (!response || typeof response !== "string") {
        throw new Error(
          `${providerName}ã‹ã‚‰ã®å¿œç­”ãŒä¸æ­£ã§ã™ã€‚ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚`
        );
      }

      if (response.length === 0) {
        throw new Error(
          `${providerName}ã‹ã‚‰ç©ºã®å¿œç­”ãŒè¿”ã•ã‚Œã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚`
        );
      }

      logDebug("AI Service", `LangGraphã‹ã‚‰å¿œç­”ã‚’å—ä¿¡ (${providerName})`, {
        provider: provider,
        responseLength: response.length,
      });

      return response;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      logError("AI Service", error, {
        provider: provider,
        attemptedAction: "chat",
        messageCount: messages.length,
        model: this.model,
      });
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚ˆã‚Šè©³ç´°ã«
      if (errorMessage.includes("fetch") || errorMessage.includes("network")) {
        throw new Error(
          `ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: ${providerName}ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n` +
          `ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n` +
          `è©³ç´°: ${errorMessage}`
        );
      }
      
      // å…ƒã®ã‚¨ãƒ©ãƒ¼ã‚’ãã®ã¾ã¾æŠ•ã’ã‚‹ï¼ˆã™ã§ã«è©³ç´°ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
      throw error;
    }
  }

  /**
   * LangGraphã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªå¯¾è©±å‡¦ç†
   */
  async chatWithWorkflow(messages: Message[]): Promise<string> {
    // ç¾åœ¨ã¯chat()ãŒLangGraphã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€åŒã˜å‡¦ç†ã‚’å‘¼ã³å‡ºã™
    return this.chat(messages);
  }
}

export const openAIService = new OpenAIService();

/**
 * ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§OpenAIServiceã‚’ä½œæˆï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰
 */
export function createAIService(
  provider: "openai" | "gemini",
  apiKey: string,
  model: string,
  temperature?: number,
  maxTokens?: number
): OpenAIService {
  const service = new OpenAIService(apiKey, model);
  if (temperature !== undefined) {
    service.setTemperature(temperature);
  }
  if (maxTokens !== undefined) {
    service.setMaxTokens(maxTokens);
  }
  return service;
}

// å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
export const createOpenAIService = createAIService;
